{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d54ece0",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 自动微分\n",
    ":label:`sec_autograd`\n",
    "\n",
    "正如我们在 :numref:`sec_calculus`中所说的那样，求导是几乎所有深度学习优化算法的关键步骤。\n",
    "虽然求导的计算很简单，只需要一些基本的微积分。\n",
    "但对于复杂的模型，手工进行更新是一件很痛苦的事情（而且经常容易出错）。\n",
    "\n",
    "深度学习框架通过自动计算导数，即*自动微分*（automatic differentiation）来加快求导。\n",
    "实际中，根据我们设计的模型，系统会构建一个*计算图*（computational graph），\n",
    "来跟踪计算是哪些数据通过哪些操作组合起来产生输出。\n",
    "自动微分使系统能够随后反向传播梯度。\n",
    "这里，*反向传播*（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。\n",
    "\n",
    "**梯度的定义**：\n",
    "函数在某点的梯度是一个向量——**方向与取得最大方向导数的方向一致,而它的模为方向导数的最大值**\n",
    "\n",
    "## 一个简单的例子\n",
    "\n",
    "作为一个演示例子，(**假设我们想对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导**)。\n",
    "首先，我们创建变量`x`并为其分配一个初始值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f462b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:05.865562Z",
     "iopub.status.busy": "2022-07-31T02:37:05.864988Z",
     "iopub.status.idle": "2022-07-31T02:37:06.545457Z",
     "shell.execute_reply": "2022-07-31T02:37:06.544791Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d985f146",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "[**在我们计算$y$关于$\\mathbf{x}$的梯度之前，我们需要一个地方来存储梯度。**]\n",
    "重要的是，我们不会在每次对一个参数求导时都分配新的内存。\n",
    "因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。\n",
    "注意，一个标量函数关于向量$\\mathbf{x}$的梯度是向量，并且与$\\mathbf{x}$具有相同的形状。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a89d68f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.548917Z",
     "iopub.status.busy": "2022-07-31T02:37:06.548534Z",
     "iopub.status.idle": "2022-07-31T02:37:06.552043Z",
     "shell.execute_reply": "2022-07-31T02:37:06.551427Z"
    },
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)，设置为需要求梯度\n",
    "x.grad  # 默认值是None，所以不输出\n",
    "\n",
    "x1 = torch.tensor([1,2,3,4,5])    #构建一个张量\n",
    "print(x1.is_leaf) #注意，jupyter中True/False这两个bool值必须打印输出\n",
    "print(x1.requires_grad)\n",
    "print(x1.grad)\n",
    "print(x1.grad_fn) #注意，jupyter中None必须打印输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11394bbe",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "pytorch的计算图里只有两种元素：数据（tensor）和 运算（operation）\n",
    "运算包括加减乘除、开方、幂指对、三角函数等可**求导运算**等\n",
    "数据可分为：叶子节点（leaf node）和非叶子节点：叶子节点是用户创建的节点，不依赖其它节点。二者的区别：反向传播结束之后，非叶子节点的梯度会被释放掉，**只保留叶子节点的梯度，节省了内存**。如果想要保留非叶子节点的梯度，可以使用retain_grad()方法。\n",
    "torch.tensor 具有如下属性（注意是属性，不是方法调用）：\n",
    "1. 查看 是否可以求导 requires_grad\n",
    "2. 查看 运算名称 grad_fn\n",
    "3. 查看 是否为叶子节点 is_leaf\n",
    "4. 查看 导数值 grad\n",
    "针对requires_grad属性，自己定义的叶子节点默认为False，而非叶子节点默认为True，神经网络中的权重默认为True。判断哪些节点是True/False的一个原则就是从你需要求导的叶子节点到loss节点之间是一条可求导的通路。\n",
    "Tensor是autograd包的核心类，若将其属性.requires_grad设置为True，它将开始追踪在其上的所有操作。完成计算后，可以调用 .backward()来完成所有梯度计算。此Tensor的梯度将累计到.grad属性中。若要停止追踪，则方法如下：\n",
    "- 调用.detach()\n",
    "- with torch.no_grad(): 包裹的代码块将不会被追踪\n",
    "\n",
    "(**现在让我们计算$y$。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "275face9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.554961Z",
     "iopub.status.busy": "2022-07-31T02:37:06.554622Z",
     "iopub.status.idle": "2022-07-31T02:37:06.560002Z",
     "shell.execute_reply": "2022-07-31T02:37:06.559374Z"
    },
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28., grad_fn=<MulBackward0>)\n",
      "<MulBackward0 object at 0x0000020EC362F400>\n"
     ]
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c39a2",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "`x`是一个长度为4的向量，计算`x`和`x`的点积，得到了我们赋值给`y`的标量输出。参数中的grad_fn指向Function对象，用于反向传播的梯度计算之用。\n",
    "接下来，我们[**通过调用反向传播函数来自动计算`y`关于`x`每个分量的梯度**]，并打印这些梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a562bf7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.562926Z",
     "iopub.status.busy": "2022-07-31T02:37:06.562592Z",
     "iopub.status.idle": "2022-07-31T02:37:06.629473Z",
     "shell.execute_reply": "2022-07-31T02:37:06.628828Z"
    },
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d6e3b3",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于$\\mathbf{x}$的梯度（函数在某点的梯度是一个向量——**方向与取得最大方向导数的方向一致,而它的模为方向导数的最大值**）应为$4\\mathbf{x}$。\n",
    "让我们快速验证这个梯度是否计算正确。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02955afb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.632562Z",
     "iopub.status.busy": "2022-07-31T02:37:06.632210Z",
     "iopub.status.idle": "2022-07-31T02:37:06.637390Z",
     "shell.execute_reply": "2022-07-31T02:37:06.636762Z"
    },
    "origin_pos": 18,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712e2c2c",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "[**现在让我们计算`x`的另一个函数。**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d26fa2e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.640282Z",
     "iopub.status.busy": "2022-07-31T02:37:06.639960Z",
     "iopub.status.idle": "2022-07-31T02:37:06.645418Z",
     "shell.execute_reply": "2022-07-31T02:37:06.644814Z"
    },
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
    "x.grad.zero_()#梯度初始化，防止梯度累计\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43640fcd",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "## 非标量变量的反向传播\n",
    "\n",
    "当`y`不是标量时，向量`y`关于向量`x`的导数的最自然解释是一个矩阵。\n",
    "对于高阶和高维的`y`和`x`，求导的结果可以是一个高阶张量。\n",
    "\n",
    "然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括[**深度学习中**]），\n",
    "但当我们调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。\n",
    "这里(**，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。**)\n",
    "\n",
    "torch.autograd提供了类和函数用于对任意标量函数进行求导，要想使用自动求导，只需对已有代码稍作改变——将所有的tensor包含进Variable对象中即可\n",
    "torch.autograd.backward()是计算向量或矩阵梯度的接口,参数说明:\n",
    "- variables (variable 列表) – 被求微分的叶子节点\n",
    "- grad_variables (Tensor 列表) – 对应variable的梯度。**仅当variable不是标量且需要求梯度的时候使用**。\n",
    "- retain_variables (bool) – True,计算梯度时所需要的buffer在计算完梯度后不会被释放。如果想对一个子图多次求微分的话，需要设置为True。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8702b8ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.648225Z",
     "iopub.status.busy": "2022-07-31T02:37:06.647897Z",
     "iopub.status.idle": "2022-07-31T02:37:06.653894Z",
     "shell.execute_reply": "2022-07-31T02:37:06.653269Z"
    },
    "origin_pos": 26,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14., grad_fn=<SumBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n",
    "# 在我们的例子中，我们只想求偏导数的和，所以传递一个1的梯度是合适的\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "# 等价于y.backward(torch.ones(len(x)))，对矢量y先求和再回溯每个叶子节点求梯度\n",
    "print(y.sum())\n",
    "print(type(y.sum())) #tensor(14., grad_fn=<SumBackward0>)，和是标量，因此无需参数grad_variables\n",
    "y.sum().backward()\n",
    "x.grad  #y对x的梯度，输出出来"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9925dd",
   "metadata": {},
   "source": [
    "上面的情况中，y.sum()是一个标量，接下来我们看一下当backward函数的variables参数为一个矢量时，该如何求梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bdfe5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 48.,  96., 160.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.tensor([1.0,2.0,3.0],requires_grad=True)\n",
    "y2 = (x2 + 2)**2\n",
    "z2 = 4*y2\n",
    "z2.backward(torch.tensor([2.,3.,4.]))#tensor参数是待求得x得梯度的系数\n",
    "'''\n",
    "不加tensor参数会报错：RuntimeError: grad can be implicitly created only for scalar outputs\n",
    "提示，标量输出才能反向求导\n",
    "'''\n",
    "x2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9b2fed",
   "metadata": {},
   "source": [
    "pytorch在求导过程中，可分为以下两种情况：\n",
    "1. 标量对向量求导，可以保证计算图的根节点只有一个，直接调用backward函数即可，无需`grad_tensors`参数\n",
    "2. 向量/矩阵对向量/矩阵求导，实质：先求出**雅克比矩阵**中每一个元素值的求解过程，对应计算图的求解方法，然后将这个雅克比矩阵和`grad_tensors`参数对应矩阵进行**相应点乘**，得到最终结果\n",
    "\n",
    "更多参考请见：https://wenku.baidu.com/view/24962b6a757f5acfa1c7aa00b52acfc789eb9ff5.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2665d5bb",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "## 分离计算\n",
    "\n",
    "有时，我们希望[**将某些计算移动到记录的计算图之外**]。\n",
    "例如，假设`y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。\n",
    "想象一下，我们想计算`z`关于`x`的梯度，但由于某种原因，我们**希望将`y`视为一个常数，并且只考虑到`x`在`y`被计算后发挥的作用。**\n",
    "\n",
    "在这里，我们可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值，\n",
    "但丢弃计算图中如何计算`y`的任何信息。\n",
    "换句话说，梯度不会向后流经`u`到`x`。\n",
    "因此，下面的反向传播函数计算`z=u*x`关于`x`的偏导数，同时**将`u`作为常数处理，而不是`z=x*x*x`关于`x`的偏导数**。\n",
    "\n",
    "`detach`的中文意思是分离，官方解释是返回一个新的Tensor,从当前的计算图中分离出来，需要注意的是，返回的新Tensor和原Tensor共享一个存储空间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aef1336d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.656825Z",
     "iopub.status.busy": "2022-07-31T02:37:06.656481Z",
     "iopub.status.idle": "2022-07-31T02:37:06.662411Z",
     "shell.execute_reply": "2022-07-31T02:37:06.661807Z"
    },
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2482b7e7",
   "metadata": {
    "origin_pos": 32
   },
   "source": [
    "由于记录了`y`的计算结果，我们可以随后在`y`上调用反向传播，\n",
    "得到`y=x*x`关于的`x`的导数，即`2*x`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d25d6074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.665549Z",
     "iopub.status.busy": "2022-07-31T02:37:06.665027Z",
     "iopub.status.idle": "2022-07-31T02:37:06.670720Z",
     "shell.execute_reply": "2022-07-31T02:37:06.670092Z"
    },
    "origin_pos": 34,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66954a77",
   "metadata": {
    "origin_pos": 36
   },
   "source": [
    "## Python控制流的梯度计算\n",
    "\n",
    "使用自动微分的一个好处是：\n",
    "[**即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度**]。\n",
    "在下面的代码中，`while`循环的迭代次数和`if`语句的结果都取决于输入`a`的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f3a4e8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.673776Z",
     "iopub.status.busy": "2022-07-31T02:37:06.673308Z",
     "iopub.status.idle": "2022-07-31T02:37:06.677303Z",
     "shell.execute_reply": "2022-07-31T02:37:06.676673Z"
    },
    "origin_pos": 38,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae128b06",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "让我们计算梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e25d3408",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.680229Z",
     "iopub.status.busy": "2022-07-31T02:37:06.679765Z",
     "iopub.status.idle": "2022-07-31T02:37:06.684566Z",
     "shell.execute_reply": "2022-07-31T02:37:06.683900Z"
    },
    "origin_pos": 42,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1286.3048, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(size=(), requires_grad=True) #torch.randn:用来生成随机数字的tensor，这些随机数字满足标准正态分布（0~1）\n",
    "d = f(a)\n",
    "d.backward()\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb899a",
   "metadata": {
    "origin_pos": 44
   },
   "source": [
    "我们现在可以分析上面定义的`f`函数。\n",
    "请注意，它在其输入`a`中是分段线性的。\n",
    "换言之，对于任何`a`，存在某个常量标量`k`，使得`f(a)=k*a`，其中`k`的值取决于输入`a`。\n",
    "因此，我们可以用`d/a`验证梯度是否正确。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7df0be3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.687918Z",
     "iopub.status.busy": "2022-07-31T02:37:06.687447Z",
     "iopub.status.idle": "2022-07-31T02:37:06.692256Z",
     "shell.execute_reply": "2022-07-31T02:37:06.691639Z"
    },
    "origin_pos": 46,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656f59b1",
   "metadata": {
    "origin_pos": 48
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上。然后我们记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 为什么计算二阶导数比一阶导数的开销要更大？ 答：因为二阶导数的计算是在一阶导数的计算基础上进行的\n",
    "1. 在运行反向传播函数之后，立即再次运行它，看看会发生什么。\n",
    "答：调用backward（）进行第二次反向传播时，计算图中保存的中间值（这里我的理解是第一次求导后的值）已经被释放掉了，所以报错解决的方案：在backward()函数中设置参数`retain_graph=True`\n",
    "1. 在控制流的例子中，我们计算`d`关于`a`的导数，如果我们将变量`a`更改为随机向量或矩阵，会发生什么？\n",
    "1. 重新设计一个求控制流梯度的例子，运行并分析结果。\n",
    "1. 使$f(x)=\\sin(x)$，绘制$f(x)$和$\\frac{df(x)}{dx}$的图像，其中后者不使用$f'(x)=\\cos(x)$。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "edd377e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.], requires_grad=True)\n",
      "tensor(28., grad_fn=<MulBackward0>)\n",
      "tensor([ 0.,  4.,  8., 12.])\n",
      "tensor([True, True, True, True])\n",
      "tensor(28., grad_fn=<MulBackward0>)\n",
      "tensor([ 0.,  4.,  8., 12.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(4.,requires_grad=True)\n",
    "print(x)\n",
    "#print(x**2)\n",
    "y = 2 * torch.dot(x**2,torch.ones_like(x))\n",
    "y.backward(retain_graph=True)\n",
    "print(y)\n",
    "print(x.grad)\n",
    "print(x.grad==4*x)\n",
    "x.grad.zero_()#如果没有这一行，会导致梯度累计\n",
    "y.backward()\n",
    "print(y)\n",
    "'''\n",
    "会报错\n",
    "RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n",
    "'''\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a99037",
   "metadata": {
    "origin_pos": 50,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1759)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0d3bcf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9808],\n",
      "        [-0.1567],\n",
      "        [ 0.9062]], requires_grad=True)\n",
      "tensor([[1014.1636],\n",
      "        [ -80.2131],\n",
      "        [ 463.9852]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [102]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(a)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(d)\n\u001b[1;32m----> 7\u001b[0m \u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03md.sum().backward()\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     11\u001b[0m a\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\torch\\autograd\\__init__.py:166\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    162\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28mtuple\u001b[39m(inputs) \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[0;32m    165\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m--> 166\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\d2l-zh\\lib\\site-packages\\torch\\autograd\\__init__.py:67\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 67\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mones_like(out, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format))\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "# 第3题\n",
    "a = torch.randn(size=(3,1), requires_grad=True) #torch.randn:用来生成随机数字的tensor，这里size设置一二维长度\n",
    "d = f(a)\n",
    "print(a)\n",
    "print(d)\n",
    "'''\n",
    "d.backward() \n",
    "会报错： RuntimeError: grad can be implicitly created only for scalar outputs\n",
    "'''\n",
    "d.sum().backward()\n",
    "\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af17fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
