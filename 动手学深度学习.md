# 动手学深度学习

## 安装配置

创建环境`d2l-zh`，python选择3.8版本，命令：`conda create -n d2l-zh python=3.8 pip`

安装需要的包：`pip install jupyter d2l torch torchvision`

在网址：https://zh-v2.d2l.ai/d2l-zh.zip 下载代码，解压缩后切换到pytorch目录并执行其中章节代码。对于上传到`github`远程仓库的代码，可以使用`git diff --stat main origin/main`查看远程仓库与本地仓库代码的差异。

浪潮平台中，`Linux`断网情况下创建环境，命令：`conda create -n d2l-zh python=3.8 pip --offline`

使用浪潮平台自带的pytorch镜像，默认的Python版本（base环境下）为3.7.0，使用`pip show`命令查看，得知`torch`版本为 1.8.0+cu111，`torchvision`版本为 0.9.0+cu111。若要下载模块的whl文件进行安装，可在[pytorch官方提高的下载网站中](https://download.pytorch.org/whl/torch_stable.html)找到适合的版本并下载。

关于该课程资料的社区讨论：https://discuss.d2l.ai/c/chinese-version/16

关于pytorch框架的社区讨论：https://discuss.pytorch.org/

资料代码：https://github.com/d2l-ai/d2l-zh



## torch框架中copy_()和clone两个函数的区别

`copy_()`函数完成与`clone()`函数类似的功能，但也存在区别。调用`copy_()`的对象是目标tensor，参数是复制操作from的tensor，最后会返回目标tensor；而`clone()`的调用对象为源tensor，返回一个新tensor。当然`clone()`函数也可以采用`torch.clone()`调用，将源tensor作为参数。

```python 
#验证copy_()和clone()
import numpy
import torch
A1 = numpy.array([1,2,3,4,666,88])
A2 = torch.from_numpy(A1)
B2=A2.clone()
B1=torch.empty_like(A2).copy_(A2)#注意copy_函数有下划线
index = torch.tensor([3])#tensor默认创建32位整形
A2 = A2.scatter(0,index,444)
print(A2)
index = torch.tensor([3])#tensor默认创建32位整形
B1 = B1.scatter(0,index,1111)
B2 = B2.scatter(0,index,2222)
print(A2)
print(B1)
print(B2)
'''
tensor([  1,   2,   3, 444, 666,  88], dtype=torch.int32),
tensor([   1,    2,    3, 1111,  666,   88], dtype=torch.int32),
tensor([   1,    2,    3, 2222,  666,   88], dtype=torch.int32)
'''
```

可以发现，在创建操作过后，A2，B1，B2三个矩阵之间任一变化不会影响到其余两个

[torch中矩阵有多种创建方法](https://blog.csdn.net/qq_44250700/article/details/120114104)，常见的是从numpy创建，再用torch.from_numpy()函数转化过来。

[torch中修改矩阵单个元素](https://zhuanlan.zhihu.com/p/382963941)，可以使用torch.scatter:

> scatter()方法按照指定的轴方向（dim）和对应的位置关系（index）逐个填充对应的值
>
> **使用场景**
>
> 对Tensor各行的不同列赋值（可以是相同的值，也可以是多个值）
>
> **参数**
>
> - dim：指定轴方向，以二维Tensor为例：
>
> - - dim=0表示逐列进行行填充
>   - dim=1表示逐列进行行填充
>
> - index：必须是与源Tensor相同的tensor类型，按照轴方向，在源Tensor中需要填充的位置
>
> - src：用来进行填充的值：
>
> - - src为一个数时，用这个数替换所有index位置上的值
>   - src为一个Tensor时，其shape必须与index一致，src中的元素会按顺序填充至对应index的位置上

pytorch中，最基础的就是[张量的各种创建及变化方法](https://blog.csdn.net/qq_44250700/article/details/120114104)，我们这里使用常见的从numpy队列转化的方式得到张量。



## 学习重点：前向传递和后向传递

该[项目](https://github.com/bitcarmanlee/easy-algorithm-interview-and-practice)中提及了多个算法，其中就包括**前向传播算法(Forward propagation)与反向传播算法(Back propagation)**，附上[文章讲解片段](https://blog.csdn.net/bitcarmanlee/article/details/78819025)：

> BackPropagation算法是多层神经网络的训练中举足轻重的算法。简单的理解，它的确就是复合函数的链式法则，但其在实际运算中的意义比链式法则要大的多。
>
> 机器学习可以看做是数理统计的一个应用，在数理统计中一个常见的任务就是拟合，也就是给定一些样本点，用合适的曲线揭示这些样本点随着自变量的变化关系.
>
> 深度学习同样也是为了这个目的，只不过此时，样本点不再限定为(x, y)点对，而可以是由向量、矩阵等等组成的广义点对(X,Y)。而此时，(X,Y)之间的关系也变得十分复杂，不太可能用一个简单函数表示。然而，人们发现可以用多层神经网络来表示这样的关系，而多层神经网络的本质就是一个多层复合的函数
>
> 和直线拟合一样，深度学习的训练也有一个目标函数，这个目标函数定义了什么样的参数才算一组“好参数”，不过在机器学习中，一般是采用成本函数（cost function），然后，训练目标就是通过调整每一个权值Wij来使得cost达到最小。cost函数也可以看成是由所有待求权值Wij为自变量的复合函数，而且基本上是非凸的，即含有许多局部最小值。但实际中发现，采用我们常用的梯度下降法就可以有效的求解最小化cost函数的问题。
>
> 梯度下降法需要给定一个初始点，并求出该点的梯度向量，然后以负梯度方向为搜索方向，以一定的步长进行搜索，从而确定下一个迭代点，再计算该新的梯度方向，如此重复直到cost收敛。那么如何计算梯度呢？
>
> 如果使用链式法则+前向传播的方式，会导致**冗余的计算**，原因在于很多路径被重复访问了，对于权值动则数万的深度模型中的神经网络，这样的冗余所导致的计算量是相当大的。
>
> 同样是利用链式法则，BP算法则机智地避开了这种冗余，它对于每一个路径只访问一次就能求顶点对所有下层节点的偏导值。
> 正如[反向传播](https://so.csdn.net/so/search?q=反向传播&spm=1001.2101.3001.7020)(BP)算法的名字说的那样，BP算法是反向(自上往下)来寻找路径的。
>
> 从最上层的节点e开始，初始值为1，以层为单位进行处理。对于e的下一层的所有子节点，将1乘以e到某个节点路径上的偏导值，并将结果“堆放”在该子节点中。等e所在的层按照这样传播完毕后，第二层的每一个节点都“堆放"些值，然后我们针对每个节点，把它里面所有“堆放”的值求和，就得到了顶点e对该节点的偏导。然后将这些第二层的节点各自作为起始顶点，初始值设为顶点e对它们的偏导值，以"层"为单位重复上述传播过程，即可求出顶点e对每一层节点的偏导数。

### 二者复杂度区别

这里也是一个常见考点，对于计算复杂度来说，正向传播和反向传播均为O(n)。

但在内存复杂度方面，由于反向传播需要保留正向传播时所有的中间结果，所以需要O(n)——导致神经网络特别耗GPU资源（爆显存）

正向传播内存复杂度为O(1)，但每计算一个变量的梯度都要扫一遍。

反向传播从根节点向下扫，可以保证每个节点只扫一次（在计算一个变量梯度时不用管同层的其他变量）；正向传播从叶子节点向上扫，会导致上层节点可能会计算多次。





## 用带有噪声的线性模型构造一个人造数据集

生成一个包含1000个样本的数据集，每个样本包含从标准正态分布中采样的2个特征。
我们的合成数据集是一个矩阵 $\mathbf{X}\in \mathbb{R}^{1000 \times 2}$ 。如果在typora中无法正常显示公式块，请在"Preference"中找到"Markdown"勾选公式块的关联项。

(**我们使用线性模型参数$\mathbf{w} = [2, -3.4]^\top$、$b = 4.2$和噪声项$\epsilon$生成数据集及其标签： $$\mathbf{y}= \mathbf{X} \mathbf{w} + b + \mathbf\epsilon.$$**)

可以将$\epsilon$视为模型预测和标签时的潜在观测误差。在这里我们认为标准假设成立，即$\epsilon$服从均值为0的正态分布。
为了简化问题，我们将标准差设为0.01，用下面的代码生成合成数据集：

```python
def synthetic_data(w, b, num_examples):  #@save
    """生成y=Xw+b+噪声"""
    X = torch.normal(0, 1, (num_examples, len(w)))
    #torch.normal函数返回从单独的正态分布中提取的随机数的张量，三个参数分别为均值，标准差和size
    y = torch.matmul(X, w) + b    #对应 X×w+b 这一步运算
    '''
    torch.matmul是tensor的乘法，输入可以是高维的。
    当输入是都是二维时，就是普通的矩阵乘法，和tensor.mm函数用法相同；
    当输入有多维时，把多出的一维作为batch提出来，其他部分做矩阵乘法。
    '''
    y += torch.normal(0, 0.01, y.shape)    #加上一个值较小的噪声项
    return X, y.reshape((-1, 1))
```

**torch.reshape用来改变tensor的shape**，值得注意的是修改的shape必须满足原来的tensor和reshape的tensor元素个数相等，我们只需做个小小的验证，比如原来tensor的shape为（2，2，3），元素个数为12，那么要进行reshape必须满足元素个数为12，如（4，3，1），（3，2，2）。

当reshape中某一个参数为-1时，-1代表n，即tensor的长度/第一个参数。比如对于一个一维长度为3，二维长度为4的向量a，`b = a.reshape(-1,1)`，那么b的一维长度就为12，二维长度为1（注意是12个长度为1的list而不是12个元素），如果`b =  a.reshape(1,-1)`，结果就是1×12的张量。
